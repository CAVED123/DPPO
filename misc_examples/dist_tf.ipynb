{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dist_tf.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EcEX8ktWeZvP","colab_type":"code","outputId":"9c9ca988-1c4e-4338-e351-7da43557e731","executionInfo":{"status":"ok","timestamp":1559483802245,"user_tz":-480,"elapsed":894,"user":{"displayName":"H C","photoUrl":"https://lh5.googleusercontent.com/-CRr1M6G7Q8s/AAAAAAAAAAI/AAAAAAAAABo/s1oBNc9JuHw/s64/photo.jpg","userId":"02161151882970450665"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Start a TensorFlow server as a single-process \"cluster\".\n","#$ python\n","import tensorflow as tf\n","c = tf.constant(\"Hello, distributed TensorFlow!\")\n","server = tf.train.Server.create_local_server()\n","sess = tf.Session(server.target)  # Create a session on the server.\n","sess.run(c)\n","#'Hello, distributed TensorFlow!'"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["b'Hello, distributed TensorFlow!'"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"eYXfUxGLVzcK","colab_type":"code","outputId":"96fc5e5c-35b0-4f54-df65-379f9ea8dd61","executionInfo":{"status":"ok","timestamp":1560090310820,"user_tz":-480,"elapsed":19429,"user":{"displayName":"H C","photoUrl":"https://lh5.googleusercontent.com/-CRr1M6G7Q8s/AAAAAAAAAAI/AAAAAAAAABo/s1oBNc9JuHw/s64/photo.jpg","userId":"02161151882970450665"}},"colab":{"base_uri":"https://localhost:8080/","height":734}},"source":["import tensorflow as tf\n","from multiprocessing import Process\n","from time import sleep\n","\n","cluster = tf.train.ClusterSpec({\n","    \"worker\": [\n","        \"localhost:2223\",\n","        \"localhost:2224\",\n","    ],\n","    \"ps\": [\n","        \"localhost:2225\"\n","    ]\n","})\n","\n","def parameter_server():\n","    with tf.device(\"/job:ps/task:0\"):\n","        var = tf.Variable(0.0, name='var')\n","\n","    server = tf.train.Server(cluster,\n","                             job_name=\"ps\",\n","                             task_index=0)\n","    sess = tf.Session(target=server.target)\n","    \n","    print(\"Parameter server: waiting for cluster connection...\")\n","    sess.run(tf.report_uninitialized_variables())\n","    print(\"Parameter server: cluster ready!\")\n","    \n","    print(\"Parameter server: initializing variables...\")\n","    sess.run(tf.global_variables_initializer())\n","    print(\"Parameter server: variables initialized\")\n","    \n","    for i in range(10):\n","        val = sess.run(var)\n","        print(\"Parameter server: var has value %.1f\" % val)\n","        sleep(1.0)\n","        if val == 10.0:\n","          break\n","\n","    #print(\"Parameter server: blocking...\")\n","    #server.join() # currently blocks forever    \n","    print(\"Parameter server: ended...\")\n","\n","def worker(worker_n):\n","    with tf.device(\"/job:ps/task:0\"):\n","        var = tf.Variable(0.0, name='var')\n","        \n","    server = tf.train.Server(cluster,\n","                             job_name=\"worker\",\n","                             task_index=worker_n)\n","    sess = tf.Session(target=server.target)\n","    \n","    print(\"Worker %d: waiting for cluster connection...\" % worker_n)\n","    sess.run(tf.report_uninitialized_variables())\n","    print(\"Worker %d: cluster ready!\" % worker_n)\n","    \n","    while sess.run(tf.report_uninitialized_variables()):\n","        print(\"Worker %d: waiting for variable initialization...\" % worker_n)\n","        sleep(1.0)\n","    print(\"Worker %d: variables initialized\" % worker_n)\n","    \n","    for i in range(5):\n","        print(\"Worker %d: incrementing var\" % worker_n)\n","        sess.run(var.assign_add(1.0))\n","        \n","        #aList.append(var)\n","        \n","        sleep(1.0)\n","    \n","    #print(\"Worker %d: blocking...\" % worker_n)\n","    #server.join() # currently blocks forever\n","    print(\"Worker %d: ended...\" % worker_n)\n","    \n","#aList = []\n","\n","ps_proc = Process(target=parameter_server, daemon=True)\n","w1_proc = Process(target=worker, args=(0, ), daemon=True)\n","w2_proc = Process(target=worker, args=(1, ), daemon=True)\n","\n","ps_proc.start()\n","w1_proc.start()\n","w2_proc.start()\n","\n","# if not join, parent will terminate before children \n","# & children will terminate as well cuz children are daemon\n","ps_proc.join() \n","#w1_proc.join()\n","#w2_proc.join()\n","\n","for proc in [w1_proc, w2_proc, ps_proc]:\n","    proc.terminate() # only way to kill server is to kill it's process\n","    \n","print('All done.')    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","Worker 0: waiting for cluster connection...\n","Worker 1: waiting for cluster connection...\n","Parameter server: waiting for cluster connection...\n","Worker 1: cluster ready!\n","Parameter server: cluster ready!\n","Parameter server: initializing variables...\n","Parameter server: variables initialized\n","Parameter server: var has value 0.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"],"name":"stderr"},{"output_type":"stream","text":["Worker 1: variables initialized\n","Worker 1: incrementing var\n","Worker 0: cluster ready!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"],"name":"stderr"},{"output_type":"stream","text":["Worker 0: variables initialized\n","Worker 0: incrementing var\n","Parameter server: var has value 2.0\n","Worker 1: incrementing var\n","Worker 0: incrementing var\n","Parameter server: var has value 4.0\n","Worker 1: incrementing var\n","Worker 0: incrementing var\n","Parameter server: var has value 6.0\n","Worker 1: incrementing var\n","Worker 0: incrementing var\n","Parameter server: var has value 8.0\n","Worker 1: incrementing var\n","Worker 0: incrementing var\n","Parameter server: var has value 10.0\n","Worker 1: ended...\n","Worker 0: ended...\n","Parameter server: ended...\n","All done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gnQ1dxppaSQk","colab_type":"code","outputId":"144aa3fc-d243-46f2-bfe7-e2a0701e3bdb","executionInfo":{"status":"ok","timestamp":1560086814531,"user_tz":-480,"elapsed":19591,"user":{"displayName":"H C","photoUrl":"https://lh5.googleusercontent.com/-CRr1M6G7Q8s/AAAAAAAAAAI/AAAAAAAAABo/s1oBNc9JuHw/s64/photo.jpg","userId":"02161151882970450665"}},"colab":{"base_uri":"https://localhost:8080/","height":768}},"source":["import tensorflow as tf\n","import numpy as np\n","from multiprocessing import Process, Queue\n","from time import sleep\n","\n","cluster = tf.train.ClusterSpec({\n","    \"worker\": [\"localhost:2223\", \"localhost:2224\"],\n","    \"ps\": [\"localhost:2225\"]\n","})\n","\n","class Obj(object):\n","  def __init__(self):   \n","    self.aVar = tf.Variable(0.0, name='var')\n","  \n","  def get_var(self):\n","    return self.aVar\n","\n","class Server(object):\n","  #def __init__(self):   \n","      \n","  def parameter_server(self):\n","    with tf.device(\"/job:ps/task:0\"):\n","      #var = tf.Variable(0.0, name='var')\n","      aObj = Obj()\n","      \n","    server = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\n","    sess = tf.Session(target=server.target)\n","    \n","    print(\"Parameter server: waiting for cluster connection...\")\n","    sess.run(tf.report_uninitialized_variables())\n","    print(\"Parameter server: cluster ready!\")\n","    \n","    print(\"Parameter server: initializing variables...\")\n","    sess.run(tf.global_variables_initializer())\n","    print(\"Parameter server: variables initialized\")\n","    \n","    for i in range(10):\n","      #val = sess.run(var)\n","      val = sess.run(aObj.get_var())\n","      print(\"Parameter server: var has value %.1f\" % val)\n","      sleep(1.0)\n","      if val == 10.0:\n","        break\n","\n","    #print(\"Parameter server: blocking...\")\n","    #server.join() # currently blocks forever    \n","    print(\"Parameter server: ended...\")\n","      \n","class Worker(object):\n","  #def __init__(self):   \n","        \n","  def worker(self, worker_n):\n","    with tf.device(\"/job:ps/task:0\"):\n","      #var = tf.Variable(0.0, name='var')\n","      aObj = Obj()\n","      \n","    server = tf.train.Server(cluster, job_name=\"worker\", task_index=worker_n)\n","    sess = tf.Session(target=server.target)\n","    \n","    print(\"Worker %d: waiting for cluster connection...\" % worker_n)\n","    sess.run(tf.report_uninitialized_variables())\n","    print(\"Worker %d: cluster ready!\" % worker_n)\n","    \n","    while sess.run(tf.report_uninitialized_variables()):\n","      print(\"Worker %d: waiting for variable initialization...\" % worker_n)\n","      sleep(1.0)\n","    print(\"Worker %d: variables initialized\" % worker_n)\n","    \n","    for i in range(5):\n","      print(\"Worker %d: incrementing var\" % worker_n)\n","      #sess.run(var.assign_add(1.0))\n","      sess.run(aObj.get_var().assign_add(1.0))\n","      sleep(1.0)\n","    \n","    #print(\"Worker %d: blocking...\" % worker_n)\n","    #server.join() # currently blocks forever\n","    print(\"Worker %d: ended...\" % worker_n)   \n","         \n","s = Server()\n","w1 = Worker()\n","w2 = Worker()\n","\n","ps_proc = Process(target=s.parameter_server, daemon=True)\n","w1_proc = Process(target=w1.worker, args=(0, ), daemon=True)\n","w2_proc = Process(target=w2.worker, args=(1, ), daemon=True)\n","\n","ps_proc.start()\n","w1_proc.start()\n","w2_proc.start()\n","\n","# if not joining, parent will terminate before children \n","# & children will terminate as well cuz children are daemon\n","ps_proc.join() \n","#w1_proc.join()\n","#w2_proc.join()\n","\n","for proc in [w1_proc, w2_proc, ps_proc]:\n","    proc.terminate() # only way to kill server is to kill it's process\n","    \n","print('done')    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","\n","Parameter server: waiting for cluster connection...\n","Worker 0: waiting for cluster connection...\n","Worker 1: waiting for cluster connection...\n","Parameter server: cluster ready!\n","Worker 1: cluster ready!\n","Parameter server: initializing variables...\n","Worker 0: cluster ready!\n","Worker 1: waiting for variable initialization...\n","Parameter server: variables initialized\n","Parameter server: var has value 0.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"],"name":"stderr"},{"output_type":"stream","text":["Worker 0: variables initialized\n","Worker 0: incrementing var\n","Parameter server: var has value 1.0\n","Worker 0: incrementing var\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"],"name":"stderr"},{"output_type":"stream","text":["Worker 1: variables initialized\n","Worker 1: incrementing var\n","Parameter server: var has value 3.0\n","Worker 0: incrementing var\n","Worker 1: incrementing var\n","Parameter server: var has value 5.0\n","Worker 0: incrementing var\n","Worker 1: incrementing var\n","Parameter server: var has value 7.0\n","Worker 0: incrementing var\n","Worker 1: incrementing var\n","Parameter server: var has value 9.0\n","Worker 1: incrementing var\n","Worker 0: ended...\n","Parameter server: var has value 10.0\n","Worker 1: ended...\n","Parameter server: ended...\n","done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lih6RiMej7_C","colab_type":"code","outputId":"901cb560-b21c-44c9-ff5a-da94923b0307","executionInfo":{"status":"ok","timestamp":1560151189623,"user_tz":-480,"elapsed":21851,"user":{"displayName":"H C","photoUrl":"https://lh5.googleusercontent.com/-CRr1M6G7Q8s/AAAAAAAAAAI/AAAAAAAAABo/s1oBNc9JuHw/s64/photo.jpg","userId":"02161151882970450665"}},"colab":{"base_uri":"https://localhost:8080/","height":921}},"source":["import tensorflow as tf\n","from multiprocessing import Process\n","from time import sleep\n","\n","cluster = tf.train.ClusterSpec({\n","    \"worker\": [\"localhost:2223\",\n","               \"localhost:2224\"\n","              ],\n","    \"ps\": [\"localhost:2225\"]\n","})\n","\n","def parameter_server():\n","    with tf.device(\"/job:ps/task:0\"):\n","        var = tf.Variable(0.0, name='var')        \n","        q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\") \n","        \n","    server = tf.train.Server(cluster,\n","                             job_name=\"ps\",\n","                             task_index=0)\n","    sess = tf.Session(target=server.target)\n","    \n","    print(\"Parameter server: waiting for cluster connection...\")\n","    sess.run(tf.report_uninitialized_variables())\n","    print(\"Parameter server: cluster ready!\")\n","    \n","    print(\"Parameter server: initializing variables...\")\n","    sess.run(tf.global_variables_initializer())\n","    print(\"Parameter server: variables initialized\")\n","    \n","    for i in range(10):\n","        print(\"Parameter server: var has value %.1f\" % sess.run(var))\n","        #print(\"ps: r\", sess.run(qd))\n","        sleep(1.0)\n","        if sess.run(var) == 10.0:\n","          break\n","    \n","    sleep(3.0)\n","    #print(\"ps: final r\", sess.run(qd))\n","    print(\"ps q.size(): \", sess.run(q.size()))  \n","    \n","    for j in range(sess.run(q.size())):\n","        print(\"ps: r\", sess.run(q.dequeue()))\n","\n","    #print(\"Parameter server: blocking...\")\n","    #server.join() # currently blocks forever    \n","    print(\"Parameter server: ended...\")\n","\n","def worker(worker_n): \n","    with tf.device(\"/job:ps/task:0\"):\n","        q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\")     \n","    with tf.device(tf.train.replica_device_setter(\n","                        worker_device='/job:worker/task:' + str(worker_n),\n","                        cluster=cluster)):\n","        var = tf.Variable(0.0, name='var')\n","        \n","    server = tf.train.Server(cluster,\n","                             job_name=\"worker\",\n","                             task_index=worker_n)\n","    sess = tf.Session(target=server.target)\n","    \n","    print(\"Worker %d: waiting for cluster connection...\" % worker_n)\n","    sess.run(tf.report_uninitialized_variables())\n","    print(\"Worker %d: cluster ready!\" % worker_n)\n","    \n","    while sess.run(tf.report_uninitialized_variables()):\n","        print(\"Worker %d: waiting for variable initialization...\" % worker_n)\n","        sleep(1.0)\n","    print(\"Worker %d: variables initialized\" % worker_n)\n","    \n","    for i in range(5):\n","        print(\"Worker %d: incrementing var\" % worker_n, sess.run(var))\n","        sess.run(var.assign_add(1.0))\n","        qe = q.enqueue(sess.run(var))\n","        sess.run(qe)\n","        #qd = q.dequeue()\n","        #print(\"Worker %d: r\" % worker_n, sess.run(qd))\n","        sleep(1.0)\n","      \n","    #print(\"Worker %d q.size(): \" % worker_n, sess.run(q.size()))  \n","    \n","    #print(\"Worker %d: blocking...\" % worker_n)\n","    #server.join() # currently blocks forever\n","    print(\"Worker %d: ended...\" % worker_n)\n","    \n","ps_proc = Process(target=parameter_server, daemon=True)\n","w1_proc = Process(target=worker, args=(0, ), daemon=True)\n","w2_proc = Process(target=worker, args=(1, ), daemon=True)\n","\n","ps_proc.start()\n","w1_proc.start()\n","w2_proc.start()\n","\n","# if not join, parent will terminate before children \n","# & children will terminate as well cuz children are daemon\n","ps_proc.join() \n","#w1_proc.join()\n","#w2_proc.join()\n","\n","for proc in [w1_proc, w2_proc, ps_proc]:\n","    proc.terminate() # only way to kill server is to kill it's process\n","        \n","print('All done.')            "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","Parameter server: waiting for cluster connection...\n","Worker 0: waiting for cluster connection...\n","Worker 1: waiting for cluster connection...\n","Worker 1: cluster ready!\n","Worker 1: waiting for variable initialization...\n","Parameter server: cluster ready!\n","Parameter server: initializing variables...\n","Parameter server: variables initialized\n","Parameter server: var has value 0.0\n","Worker 0: cluster ready!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"],"name":"stderr"},{"output_type":"stream","text":["Worker 0: variables initialized\n","Worker 0: incrementing var 0.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"],"name":"stderr"},{"output_type":"stream","text":["Worker 1: variables initialized\n","Worker 1: incrementing var 1.0\n","Parameter server: var has value 2.0\n","Worker 0: incrementing var 2.0\n","Worker 1: incrementing var 3.0\n","Parameter server: var has value 4.0\n","Worker 0: incrementing var 4.0\n","Worker 1: incrementing var 5.0\n","Parameter server: var has value 6.0\n","Worker 0: incrementing var 6.0\n","Worker 1: incrementing var 7.0\n","Parameter server: var has value 8.0\n","Worker 0: incrementing var 8.0\n","Worker 1: incrementing var 9.0\n","Worker 0: ended...\n","Worker 1: ended...\n","ps q.size():  10\n","ps: r 1.0\n","ps: r 2.0\n","ps: r 3.0\n","ps: r 4.0\n","ps: r 5.0\n","ps: r 6.0\n","ps: r 7.0\n","ps: r 8.0\n","ps: r 9.0\n","ps: r 10.0\n","Parameter server: ended...\n","All done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GJ0L9WL6a0cY","colab_type":"code","outputId":"aaa503bc-6923-4914-a0b4-ba697991cc99","executionInfo":{"status":"ok","timestamp":1560087832887,"user_tz":-480,"elapsed":2612,"user":{"displayName":"H C","photoUrl":"https://lh5.googleusercontent.com/-CRr1M6G7Q8s/AAAAAAAAAAI/AAAAAAAAABo/s1oBNc9JuHw/s64/photo.jpg","userId":"02161151882970450665"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["import tensorflow as tf\n","\n","num_gpus = 1 #2\n","\n","# place the initial data on the cpu (same as before)\n","with tf.device('/cpu:0'):\n","    input_data = tf.Variable([[1., 2., 3.],\n","                              [4., 5., 6.],\n","                              [7., 8., 9.],\n","                              [10., 11., 12.]])\n","    b = tf.Variable([[1.], [1.], [2.]])\n","\n","# split the data into chunks for each gpu (new)\n","inputs = tf.split(input_data, num_gpus)\n","outputs = []\n","\n","# loop over available gpus and pass input data (new)\n","# copies of the same graph but receiving different data\n","for i in range(num_gpus):\n","    with tf.device('/gpu:'+str(i)):\n","        outputs.append(tf.matmul(inputs[i], b))\n","\n","# merge the results of the devices (new)\n","with tf.device('/cpu:0'):\n","    output = tf.concat(outputs, axis=0)\n","\n","# create a session and run (same as before)\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    print(sess.run(output))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","[[ 9.]\n"," [21.]\n"," [33.]\n"," [45.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"umx_9HE0e0Fe","colab_type":"code","colab":{}},"source":["import sys\n","import tensorflow as tf\n","\n","\"\"\"\n","# specify the cluster's architecture\n","cluster = tf.train.ClusterSpec({'ps': ['192.168.1.1:1111'], \n","                                'worker': ['192.168.1.2:1111',\n","                                           '192.168.1.3:1111']\n","                               })\n","\"\"\"\n","# specify the cluster's architecture\n","cluster = tf.train.ClusterSpec({'ps': ['localhost:3330'], \n","                                'worker': ['localhost:3331'\n","                                           'localhost:3332']\n","                               })\n","\n","# parse command-line to specify machine\n","job_type = sys.argv[1]  # job type: \"worker\" or \"ps\"\n","task_idx = sys.argv[2]  # index job in the worker or ps list\n","                        # as defined in the ClusterSpec\n","\n","# create TensorFlow Server. This is how the machines communicate.\n","server = tf.train.Server(cluster, job_name=job_type, task_index=task_idx)\n","\n","# parameter server is updated by remote clients. \n","# will not proceed beyond this if statement.\n","if job_type == 'ps':\n","    server.join()\n","else:\n","    # workers only\n","    with tf.device(tf.train.replica_device_setter(\n","                        worker_device='/job:worker/task:'+task_idx,\n","                        cluster=cluster)):\n","        # build your model here as if you only were using a single machine\n","        \n","    with tf.Session(server.target):\n","        # train your model here"],"execution_count":0,"outputs":[]}]}